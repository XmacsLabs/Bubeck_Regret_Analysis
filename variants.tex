In the previous chapters we explored a few fundamental variations around the basic multi-armed bandit problem. In both the stochastic and adversarial frameworks, these variants basically revolved around a single principle: by adding constraints on the losses (or rewards), it is possible to compete against larger sets of arms. While this is indeed a fundamental axis in the space of bandit problems, it is important to realize that there are many other directions. Indeed, we might sketch a ``bandit space'' spanning the following coordinates:
\begin{itemize}
\item Evolution of payoffs over time: stochastic, adversarial, Markovian, \dots
\item Structure of payoff functions: linear, Lipschitz, Gaussian process, \dots
\item Feedback structure: full information, bandit, semi-bandit, partial monitoring, \dots
\item Context structure (if any).
\item Notion of regret.
\end{itemize}
Clearly, such extensions greatly increase the number of potential applications of bandit models. While many of these extensions were already discussed in the previous chapters, in the following we focus on others (such as the sleeping bandits or the thruthful bandits) so to visit more exotic regions of the bandit space.

\section{Markov Decision Processes, restless and sleeping bandits}
\label{s:mdp}
Extending further the model of Markovian bandits (mentioned at the end of Chapter~\ref{intro}), one can also define a general Markov Decision Process (MDP) ---see also Section~\ref{s:mdp}. For example, the stochastic bandit of Chapter~\ref{stochastic} corresponds to a single-state MDP.

In full generality, a finite MDP can be described by a set of states $\{1, \hdots, S\}$, a set of actions $\{1,\hdots, K\}$, a set $\{p_{i,s},\, 1 \leq i \leq K,\, 1 \leq s \leq S\}$ of transition distributions over $S$, and a set $\{\nu_{i,s},\, 1 \leq i \leq K,\, 1 \leq s \leq S\}$ of reward distributions over $[0,1]$. In this model, taking action $i$ in state $s$ generates a stochastic reward drawn from $\nu_{i,s}$ and a Markovian transition to a state drawn from $p_{i,s}$. Similarly to the multi-armed bandit problem, here one typically assumes that the reward distributions and transition distributions are unknown, and the goal is to navigate through the MDP so as to maximize some function of the obtained rewards. The field that studies this type of problem is called Reinforcement Learning. The interested reader is addressed to \cite{SB98, Kak03, Sze10}. Reinforcement learning results with a flavor similar to those described in the previous chapters can be found in \cite{YMS09, BM10, JOA10, NGSA10}.

An intermediate model, between stochastic multi-armed bandits and MDPs, is the one of restless bandits. As in Markovian bandits, each arm is associated with a Markovian reward process with its own state space. Each time an arm is chosen, the associated Markov process generates an observable reward and makes a transition to a new state, which is also observed. However, unlike Markovian bandits an unobserved transition occurs for each arm that is not chosen. Using concentration inequalities for Markov chains ---see, e.g., \cite{Lez98}, one can basically show that, under suitable assumptions, UCB attains a logarithmic regret for restless bandits as well ---see \cite{TL11} and~\cite{filippi2011optimally}. A more general regret bound for restless bandits has been recently proven by~\cite{ortner2012regret}.

An apparently similar problem was studied by \cite{GM11}, where they assume that the reward distributions can abruptly change at unknown time instants (and there is a small number of such change-points). Within this model, the authors prove that the best possible regret is of order $\sqrt{n}$, which is matched by the Exp3.P algorithm ---see the discussion in Section \ref{sec:exp3S}. Thus, while the two problems (restless bandits and bandits with change-points) might look similar, they are fundamentally different. In particular, note that the latter problem cannot be cast as a MDP.

Another intermediate model, with important applications, is that of the sleeping bandits. There, it is assumed that the set of available actions is varying over time. We refer the interested reader to \cite{KNS10, KMB09, slivkins2009contextual, KS12} for the details of this model as well as the theoretical guarantees that can be obtained. A somewhat related problem was also studied in \cite{GKSS07} where it is assumed that the set of arms becomes unavailable for a random time after each arm pull (and the distribution of this random time depends on the selected arm).

\section{Pure exploration problems}
The focus of bandits, and most of their variants, is on problems where there is a notion of cumulative rewards, which is to be maximized. This criterion leaves out a number of important applications where there is an online aspect (e.g., sequential decisions), but the goal is not maximizing cumulative rewards. The simplest example is perhaps the pure exploration version of stochastic bandits. In this model, at the end of round $n$ the algorithm has to output a recommendation $J_n$ which represents its estimate for the optimal arm. The focus here is on the control of the so-called simple regret, introduced by \cite{BMS09, BMS11} and defined as $r_n = \mu^* - \E\,\mu_{J_n}$.

\cite{BMS09} prove that minimizing the simple regret is fundamentally different from minimizing the pseudo-regret $\oR_n$, in the sense that one always have $r_n \geq \exp(- C \oR_n)$ for some constant $C>0$ (which depends on the reward distributions). Thus, this regret calls for different bandit algorithms. \cite{ABM10} exhibit a simple strategy with optimal performances up to a logarithmic factor. The idea is very simple: the strategy SR (Successive Rejects) works in $K-1$ phases. SR keeps a set of active arms, that are sampled uniformly in each phase. At the end of a phase, the arm with smallest empirical mean is removed from the set of active arms. It can be shown that this strategy has a simple regret of order $\exp\left(- c \frac{n}{H \ln K}\right)$, where $H = \sum_{i \neq i^*} \frac{1}{\Delta_i^2}$ is the complexity measure of identifying the best arm, and $c$ is a numerical constant. Moreover, a matching lower bound (up to logarithmic factors) was also proved. These ideas were extended in different ways by \cite{GGLB11, BJM11, BWV12}.

A similar problem was studied in a PAC model by \cite{EMM02}. The goal is to find, with probability at least $1-\delta$, an arm with mean at least $\epsilon$ close the optimal mean, and the relevant quantity is the number of pulls needed to achieve this goal. For this problem, the authors derive an algorithm called Successive Elimination that achieves an optimal number of pulls (up to logarithmic factors). Successive Elimination works as follows: it keeps an estimate of the mean of each arm, together with a confidence interval. If two confidence intervals are disjoint, then the arm with the lowest confidence interval is eliminated. Using this procedure, one can achieve the $(\epsilon, \delta)$ guarantee with a number of pulls of order $H \ln \tfrac{K}{\Delta}$. A matching lower bound is due to \cite{MT04}, and further results are discussed by \cite{EMM06}.

In some applications one is not interested in the best arm, but rather in having a good estimate of the mean $\mu_i$ for each arm. In this setting a reasonable measure of performance is given by
$$L_n = \E \sum_{i=1}^K \bigl(\mu_i - \hat{\mu}_{i,T_i(n)}\bigr)^2~.$$
Clearly, the optimal static allocation depends only on the variances of the arms, and we denote by $L_n^*$ the performance of this strategy. 
This setting was introduced by \cite{AGS08}, where the authors studied the regret $L_n - L_n^*$, and showed that a regret of order $n^{-3/2}$ was achievable. This result was then refined by \cite{CLGMA11, CM11}. The basic idea in these papers is to resort to the optimism in face of uncertainty principle, and to approximate the optimal static allocation by replacing the true variance with an upper confidence bound on it.
%\cite{RP11}

\section{Dueling bandits} \label{sec:dueling}
An interesting variation of stochastic bandits was recently studied by \cite{YBKJ09}. The model considered in that paper is called dueling bandits. The main idea is that the player has to choose a pair or arms $(I_t, J_t)$ at each round, and can only observe the relative performances of these two arms, i.e., the player only knows which arm had the highest reward. More formally, in dueling bandits we assume that there exists a total ordering $\succ$ on $\{1,\hdots,K\}$ with the following properties:
\begin{enumerate}
\item If $i \succ j$, then the probability that the reward of arm $i$ is larger than the reward of arm $j$ is equal to $\frac12 + \Delta_{i,j}$ with $\Delta_{i,j} >0$.
\item If $i \succ j \succ k$, then $\Delta_{i,j} + \Delta_{j,k} \geq \Delta_{i,k} \geq \max\bigl\{\Delta_{i,j}, \Delta_{j,k}\bigr\}$.
\end{enumerate}
Upon selecting a pair $(I_t, J_t)$, the player receives a random variable drawn from a Bernoulli distribution with parameter $\frac12+\Delta_{i,j}$. In this setting a natural regret notion is the following quantity, where $i^*$ is the largest element in the ordering $\succ$,
$$\E \sum_{t=1}^n \bigl(\Delta_{i^*, I_t} + \Delta_{i^*, J_t} \bigr) .$$
It was proved in \cite{YBKJ09} that the optimal regret for this problem is of order $\frac{K}{\Delta} \log n$, where $\Delta = \min_{i \neq j} \Delta_{i,j}$. A simple strategy that attains this rate, based on the Successive Elimination algorithm of \cite{EMM02}, was proposed by \cite{YJ11}.
%The strategy is based on the Successive Elimination algorithm of \cite{EMM02}: the algorithm keeps a set of active arms $A_t$, and has an estimate for each arm $i$ of $\sum_{j \in A_t} \Delta_{i,j}$, together with confidence intervals for these estimates. When one of the confidence interval is disjoint from the others, the algorithm eliminates the corresponding arm from the set of active arms. We refer the reader to \cite{YJ11} for more details.

\section{Discovery with probabilistic expert advice}
\cite{BEG11} study a model with a stochastic bandit flavor (in fact it can be cast as an MDP), where the key for the analysis is a sort of 'non-linear' regret bound.
% optimality is not a regret notion, but rather what they call a ``macroscopic limit''. 
In this model rewards represent items in some set $\cX$ which is partitioned in a subset  $A \subset \cX$ of interesting items and in a subset $\cX\setminus A$ of non-interesting items. The goal is to maximize the total expected number of interesting items found after $n$ pulls, where observing twice the same item does not help. A natural notion of regret is obtained by comparing the number of interesting items $F(n)$ found by a given strategy to the number $F^*(n)$ found by the optimal strategy. It turns out that analyzing such regret directly is difficult. The first issue is that in this problem the notion of a ``good'' arm is dynamic, in the sense that an arm could be very good for a few pulls and then completely useless. Furthermore a strategy making bad decisions in the beginning will have better opportunities in the future than the optimal strategy (which already discovered some interesting items). Taking into account these issues, it turns out that it is easier to show that for good strategies, $F(n)$ is not too far from $F^*(n')$, where $n'$ is not much smaller than $n$. Such a statement -- which can be interepreted as a non-linear regret bound -- shows that the analyzed strategy slightly 'lags' behind the optimal strategy. In \cite{BEG11} a non-linear regret bound is derived for an algorithm based on estimating the mass of interesting items left on each arm (the so-called Good-Turing estimator), combined with the optimism in face of uncertainty principle of Chapter \ref{stochastic}. We refer the reader to \cite{BEG11} for more precise statements.
%is a very hard task. Indeed, in this problem the notion of a ``good'' arm is dynamic, in the sense that an arm could be very good for a few pulls and then completely useless. The authors address this issue by introducing the concept of a macrosopic limit, where both the number of rounds and the size of the problem (basically, the cardinalities of $A$ and $\cX$) go to infinity. In the limit, they show that an algorithm based on estimating the mass of interesting items left on each arm (the so-called Good-Turing estimator), combined with the optimism in face of uncertainty principle, yields a strategy with vanishing regret. We refer the reader to \cite{BEG11} for more precise statements.

\section{Many-armed bandits}
The many-armed bandit setting was introduced by \cite{BCZHS97}, and then extended and refined by \cite{WAM08}. This setting corresponds to a stochastic bandit with an infinite number of arms. The extra assumption that makes this problem feasible is a prior knowledge on the distribution of the arms. More precisely, when the player ask to ``add'' a new arm to his current set of active arms, one assumes that the probability that this arm is $\epsilon$-optimal is of order $\epsilon^{\beta}$, for some known $\beta > 0$. Thus the player faces a trade-off between exploitation, exploration, and discovery, where the last component comes from the fact that the player needs to consider new arms to make sure that he has an active $\epsilon$-optimal arm. Using a UCB strategy on the active arms, and by adding new arms at a rate which depends on $\beta$, \cite{WAM08} prove that a regret of order
\[
    n^{\max \bigl\{\frac12,\, \frac{\beta}{1+\beta}\bigr\}}
\]
is achievable in this setting.

\section{Truthful bandits} \label{sec:truthful}
A popular application domain for bandit algorithms is ad placement on the Web. In the pay-per-click model, for each incoming user $t=1,2,\dots$ the publisher selects an advertiser $I_t$ from a pool of $K$ advertisers, and display the corresponding ad to the user. The publisher then gets a reward if the ad is clicked by the user. This problem is well modeled by the multi-armed bandit setting. However, there is a fundamental aspect of the ad placement process which is overlooked by this formulation. Indeed, prior to running an ad-selection algorithm (i.e., a bandit algorithm), each advertiser $i \in \{1, \hdots, K\}$ issues a bet $b_i$. This number is how much $i$ is willing to pay for a click. Each bidder keeps also a private value $v_i$, which is the true value $i$ assigns to a click. Because a rational bidder ensures that $b_i \le v_i$, the difference $v_i-b_i$ defines the utility for bidder $i$. The basic idea of {\em truthful} bandits is to construct a bandit algorithm such that each advertiser has no incentive in submitting a bet $b_i$ such that $b_i < v_i$. A natural question to ask is whether this restriction to truthful algorithms changes the dynamics of the multi-armed bandit problem. This has investigated in a number of papers, including~\cite{BSS09, DK09, BKS10, WS12}. Thruthful bandits are part of a more general thread of research at the interface between bandits and Mechanism Design.
% Suprisingly, \cite{BSS09} and~\cite{DK09} showed that the answer is yes. Indeed, the optimal regret for this problem is of order $n^{2/3}$ (whereas it is of order $\sqrt{n}$ for the standard version). We refer the reader to \cite{BSS09, DK09, BKS10} for more details on this model.


\section{Concluding remarks}
As pointed out in the introduction, the growing interest for bandits arises from the large number of industrially relevant problems that can be modeled as a multi-armed bandit. In particular, the sequential nature of the bandit setting makes it perfectly suited to various Internet and Web applications. These include search engine optimization with dueling bandits, or ad placement with contextual bandits and truthful bandits, see the references in, respectively, Section~\ref{sec:dueling}, Section~\ref{sec:contextual} and Section~\ref{sec:truthful}.

Multi-armed bandits also proved to be very useful in other areas. For example, thanks to the strong connections between bandits and Markov Decision Processes, a breakthrough in Monte Carlo Tree Search (MCTS) was achieved using bandits ideas. More precisely, based on the sparse planning idea of \cite{KMN02}, \cite{KS06} introduced a new MCTS strategy called UCT (UCB applied to Trees) that led to a substantial advancement in Computer Go performance, see \cite{GWMT06}. Note that, from a theoretical point of view UCT was proved to perform poorly by \cite{CM07}, and a strategy based on a similar idea, but with improved theoretical performance, was proposed by \cite{BM10}. Other applications in related directions have also been explored, see for example \cite{TT09, HT10b} and many others.

Many new domains of application for bandits problems are currently investigated. For example: multichannel opportunistic communications~\cite{LZK10}, model selection~\cite{ADBL11}, boosting~\cite{BK11}, management of dark pools of liquidity (a recent type of stock exchange)~\cite{ABD10}, security analysis of power systems~\cite{BEG11}.

Given the fast pace of new variants, extensions, and applications coming out every week, we had to make tough decisions about what to present in this survey. We apologize for everything we had to leave out. On the other hand, we do hope that what we decided to put in will enthuse more researchers about entering this exciting field.
