In this chapter we consider the important variant of the multi-armed bandit problem where no stochastic assumption is made on the generation of rewards. Denote by $g_{i,t}$ the reward (or gain) of arm $i$ at time step $t$. We assume all rewards are bounded, say $g_{i,t}\in [0,1]$. At each time step $t=1, 2,\hdots$, simultaneously with the player's choice of the arm $I_t \in \{1,\hdots,K\}$, an adversary assigns to each arm $i=1,\dots,K$ the reward $g_{i,t}$. Similarly to the stochastic setting, we measure the performance of the player compared to the performance of the best arm through the regret
$$R_n = \max_{i=1,\hdots,K} \sum_{t=1}^n g_{i,t} - \sum_{t=1}^n g_{I_t,t}~.$$
Sometimes we consider losses rather than gains. In this case we denote by $\ell_{i,t}$ the loss of arm $i$ at time step $t$, and the regret rewrites as
$$R_n = \sum_{t=1}^n \ell_{I_t,t} - \min_{i=1,\hdots,K} \sum_{t=1}^n \ell_{i,t}~.$$
The loss and gain versions are symmetric, in the sense that one can translate the analysis from one to the other setting via the equivalence $\ell_{i,t} = 1 - g_{i,t}$. In the following we emphasize the loss version, but we revert to the gain version whenever it makes proofs simpler.

The main goal is to achieve sublinear (in the number of rounds) bounds on the regret uniformly over all possible adversarial assignments of gains to arms. At first sight, this goal might seem hopeless. Indeed, for any deterministic forecaster there exists a sequence of losses $(\ell_{i,t})$ such that $R_n \geq n/2$. Concretely, it suffices to consider the following sequence of losses:
$$
\begin{array}{c}
\text{if} \; I_t = 1, \; \text{then} \; \ell_{2,t} = 0 \; \text{and} \; \ell_{i,t}=1 \; \text{for all} \; i \neq 2;
\\
\text{if} \; I_t \neq 1, \; \text{then} \; \ell_{1,t} = 0 \; \text{and} \; \ell_{i,t}=1 \; \text{for all} \; i \neq 1.
\end{array}
$$
The key idea to get around this difficulty is to add randomization to the selection of the action $I_t$ to play. By doing so, the forecaster can ``surprise'' the adversary, and this surprise effect suffices to get a regret essentially as low as the minimax regret for the stochastic model. Since the regret $R_n$ then becomes a random variable, the goal is thus to obtain bounds in high probability or in expectation on $R_n$ (with respect to both eventual randomization of the forecaster and of the adversary). This task is fairly difficult, and a convenient first step is to bound the pseudo-regret
\begin{equation} \label{eq:pseudoregretadv}
\oR_n = \E \sum_{t=1}^n \ell_{I_t,t} - \min_{i=1,\hdots,K} \E \sum_{t=1}^n \ell_{i,t}~.
\end{equation}
Clearly $\oR_n \leq \E\,R_n$, and thus an upper bound on the pseudo-regret does not imply a bound on the expected regret. As argued in the Introduction, the pseudo-regret has no natural interpretation unless the adversary is oblivious. In that case, the pseudo-regret coincides with the standard regret, which is always the ultimate quantity of interest.


\section{Pseudo-regret bounds}
As we pointed out, in order to obtain non-trivial regret guarantees in the adversarial framework it is necessary to consider randomized forecasters. Below we describe the randomized forecaster Exp3, which is based on two fundamental ideas.
%
%\begin{figure}[t]
\begin{center}
\bookbox{
{\em Exp3 (Exponential weights for Exploration and Exploitation)}
\\
{Parameter:} a non-increasing sequence of real numbers $(\eta_t)_{t \in \N}$.

\smallskip\noindent
Let $p_1$ be the uniform distribution over $\{1,\hdots,K\}$.

\smallskip\noindent
For each round $t=1,2,\ldots,n$
\begin{itemize}
\item[(1)]
Draw an arm $I_t$ from the probability distribution $p_t$.
\item[(2)]
For each arm $i=1,\dots,K$ compute the estimated loss $\tilde{\ell}_{i,t} = \frac{\ell_{i,t}}{p_{i,t}} \ds1_{I_t = i}$ and update the estimated cumulative loss
$\tilde{L}_{i,t} = \tilde{L}_{i,t-1} + \tilde{\ell}_{i,s}$.
\item[(3)]
Compute the new probability distribution over arms $p_{t+1}=\bigl(p_{1,t+1},\hdots,p_{K,t+1}\bigr)$, where
$$p_{i,t+1} = \frac{\exp{\left(- \eta_t \tilde{L}_{i,t}\right)}}{\sum_{k=1}^K \exp{\left(- \eta_t \tilde{L}_{k,t}\right)}}~.$$
\end{itemize}
}
%\caption{Exp3 forecaster.}
%\label{fig:exp3}
%\end{figure}
\end{center}
%
First, despite the fact that only the loss of the played arm is observed, with a simple trick it is still possible to build an unbiased estimator for the loss of any other arm. Namely, if the next arm $I_t$ to be played is drawn from a probability distribution $p_t=\bigl(p_{1,t},\hdots,p_{K,t}\bigr)$, then
\[
    \tilde{\ell}_{i,t} = \frac{\ell_{i,t}}{p_{i,t}} \ds1_{I_t=i}
\]
is an unbiased estimator (with respect to the draw of $I_t$) of $\ell_{i,t}$. Indeed, for each $i=1,\dots,K$ we have
\[
    \E_{I_t \sim p_t} \bigl[\tilde{\ell}_{i,t}\bigr] = \sum_{j=1}^K  p_{j,t}\frac{\ell_{i,t}}{p_{i,t}} \ds1_{j=i}  = \ell_{i,t}~.
\]
The second idea is to use an exponential reweighting of the cumulative estimated losses to define the probability distribution $p_t$ from which the forecaster will select the arm $I_t$. Exponential weighting schemes are a standard tool in the study of sequential prediction schemes under adversarial assumptions. The reader is referred to the monograph by \cite{CL06} for a general introduction to prediction of individual sequences, and to the recent survey by \cite{arora2012multiplicative} focussed on computer science applications of exponential weighting.
%This leads to the strategy described in Figure~\ref{fig:exp3}. 

We provide two different pseudo-regret bounds for this strategy. The bound \eqref{eq:exp3boundanytime} is obtained assuming that the forecaster does not know the number of rounds $n$. This is the anytime version of the algorithm. The bound \eqref{eq:exp3bound}, instead, shows that a better constant can be achieved using the knowledge of the time horizon.
%
\begin{theorem}[Pseudo-regret of Exp3] \label{th:Exp3}
If Exp3 is run with $\eta_t=\eta = \sqrt{\frac{2 \ln K}{n K}}$, then
\begin{equation} \label{eq:exp3bound}
\oR_n \leq \sqrt{2 n K \ln K}~.
\end{equation}
Moeover, if Exp3 is run with $\eta_t = \sqrt{\frac{\ln K}{t K}}$, then
\begin{equation} \label{eq:exp3boundanytime}
\oR_n \leq 2 \sqrt{n K \ln K}~.
\end{equation}
\end{theorem}
%
\begin{proof}
We prove that for any non-increasing sequence $(\eta_t)_{t \in \N}$ Exp3 satisfies
\begin{equation} \label{eq:exp3lem}
\oR_n \leq \frac{K}{2} \sum_{t=1}^n \eta_t + \frac{\ln K}{\eta_n}~.
\end{equation}
%
Inequality \eqref{eq:exp3bound} then trivially follows from \eqref{eq:exp3lem}. For \eqref{eq:exp3boundanytime} we use \eqref{eq:exp3lem} and
$\sum_{t=1}^n \frac{1}{\sqrt{t}} \leq \int_{0}^n \frac{1}{\sqrt{t}} dt = 2 \sqrt{n}$. The proof of \eqref{eq:exp3lem} in divided in five steps.

\subsection*{First step: Useful equalities.}
%
The following equalities can be easily verified:
\begin{equation} \label{eq:equalities}
\E_{i \sim p_t} \tilde{\ell}_{i,t} = \ell_{I_t,t}, \quad \E_{I_t \sim p_t} \tilde{\ell}_{i,t} = \ell_{i,t}, \quad
\E_{i \sim p_t} {\tilde{\ell}}^2_{i,t} = \frac{\ell_{I_t,t}^2}{p_{I_t,t}}, \quad \E_{I_t \sim p_t} \frac{1}{p_{I_t,t}} = K~.
\end{equation}
In particular, they imply
\begin{equation} \label{eq:expregret}
\sum_{t=1}^n \ell_{I_t,t} - \sum_{t=1}^n \ell_{k,t}  = \sum_{t=1}^n \E_{i \sim p_t} \tilde{\ell}_{i,t} - \sum_{t=1}^n \E_{I_t \sim p_t} \tilde{\ell}_{k,t}~.
\end{equation}
The key idea of the proof is rewrite $\E_{i \sim p_t} \tilde{\ell}_{i,t}$ as follows 
\begin{align} 
 \E_{i \sim p_t} \tilde{\ell}_{i,t} = & \frac{1}{\eta_t} \ln \E_{i \sim p_t} \exp{\left(- \eta_t \bigl(\tilde{\ell}_{i,t}  - \E_{k \sim p_t} \tilde{\ell}_{k,t}\bigr) \right)}  \notag \\
&- \frac{1}{\eta_t} \ln \E_{i \sim p_t} \exp{\left(- \eta_t \tilde{\ell}_{i,t} \right)}~. \label{eq:logmoment}
\end{align}
The reader may recognize $\ln \E_{i \sim p_t} \exp{\bigl(- \eta_t \tilde{\ell}_{i,t} \bigr)}$ as the cumulant-generating function (or the log of the moment-generating function) of the random variable $\tilde{\ell}_{I_t,t}$. This quantity naturally arises in the analysis of forecasters based on exponential weights. In the next two steps we study the two terms in the right-hand side of \eqref{eq:logmoment}.

\subsection*{Second step: Study of the first term in \eqref{eq:logmoment}.}
%
We use the inequalities $\ln x \leq x-1$ and $\exp(-x) - 1 + x \leq x^2/2$, for all $x \geq 0$, to obtain:
\begin{align}
\ln \E_{i \sim p_t} &\exp{\left(- \eta_t (\tilde{\ell}_{i,t} - \E_{k \sim p_t} \tilde{\ell}_{k,t}) \right)} \notag \\
& =  \ln \E_{i \sim p_t} \exp{\left(- \eta_t \tilde{\ell}_{i,t}\right)} + \eta_t \E_{k \sim p_t} \tilde{\ell}_{k,t} \notag \\
&\leq \E_{i \sim p_t} \left(\exp{\left(- \eta_t \tilde{\ell}_{i,t}\right)} - 1 + \eta_t \tilde{\ell}_{i,t} \right) \notag \\
&\leq \E_{i \sim p_t} \frac{\eta_t^2 \tilde{\ell}_{i,t}^2}{2} \notag \\
&\leq \frac{\eta_t^2}{2 p_{I_t,t}} \label{eq:secondstep}
\end{align}
where the last step comes from the third equality in \eqref{eq:equalities}.

\subsection*{Third step: Study of the second term in \eqref{eq:logmoment}.}
%
Let $\tilde{L}_{i,0}=0$, $\Phi_0(\eta)=0$ and $\Phi_t(\eta) = \frac{1}{\eta} \ln \frac{1}{K} \sum_{i=1}^K \exp{\left(- \eta \tilde{L}_{i,t}\right)}$. Then, by definition of $p_t$ we have
\begin{align}
- \frac{1}{\eta_t} \ln \E_{i \sim p_t} \exp{\left(- \eta_t \tilde{\ell}_{i,t} \right)} 
& = - \frac{1}{\eta_t} \ln \frac{\sum_{i=1}^K \exp{\left(- \eta_t \tilde{L}_{i,t}\right)}}{\sum_{i=1}^K \exp{\left(- \eta_t \tilde{L}_{i,t-1}\right)}} \notag \\
& = \Phi_{t-1}(\eta_t) - \Phi_{t}(\eta_t)~. \label{eq:thirdstep}
\end{align}

\subsection*{Fourth step: Summing.}
%
Putting together \eqref{eq:expregret}, \eqref{eq:logmoment}, \eqref{eq:secondstep} and \eqref{eq:thirdstep} we obtain
$$\sum_{t=1}^n g_{k,t} - \sum_{t=1}^n g_{I_t,t} \leq \sum_{t=1}^n \frac{\eta_t}{2 p_{I_t,t}} + \sum_{t=1}^n \Phi_{t-1}(\eta_t) - \Phi_t(\eta_t) - \sum_{t=1}^n \E_{I_t \sim p_t} \tilde{\ell}_{k,t}~.$$
The first term is easy to bound in expectation since, by the rule of conditional expectations and the last equality in \eqref{eq:equalities} we have
$$\E \sum_{t=1}^n \frac{\eta_t}{2 p_{I_t,t}} = \E \sum_{t=1}^n \E_{I_t \sim p_t} \frac{\eta_t}{2 p_{I_t,t}} = \frac{K}{2} \sum_{t=1}^n \eta_t~.$$
For the second term we start with an Abel transformation,
$$\sum_{t=1}^n \bigl(\Phi_{t-1}(\eta_t) - \Phi_t(\eta_t)\bigr) = \sum_{t=1}^{n-1} \bigl(\Phi_t(\eta_{t+1}) - \Phi_t(\eta_t)\bigr) - \Phi_n(\eta_n)$$
since $\Phi_0(\eta_1)=0$. Note that
\begin{align*}
- \Phi_n(\eta_n) & = \frac{\ln K}{\eta_n} - \frac{1}{\eta_n} \ln\left(\sum_{i=1}^K \exp\left(-\eta_n \tilde{L}_{i,n}\right)\right) \\ & \leq \frac{\ln K}{\eta_n} - \frac{1}{\eta_n} \ln\left(\exp\left(-\eta_n \tilde{L}_{k,n}\right)\right)\\
& =  \frac{\ln K}{\eta_n} + \sum_{t=1}^n \tilde{\ell}_{k,t}
\end{align*}
and thus we have
$$\E \left[ \sum_{t=1}^n g_{k,t} - \sum_{t=1}^n g_{I_t,t} \right] \leq \frac{K}{2} \sum_{t=1}^n \eta_t + \frac{\ln K}{\eta_n} + \E \sum_{t=1}^{n-1} \Phi_t(\eta_{t+1}) - \Phi_t(\eta_t)~.$$
To conclude the proof, we show that $\Phi_t'(\eta)\geq 0$. Since $\eta_{t+1}\leq \eta_t$, we then obtain $\Phi_t(\eta_{t+1}) - \Phi_t(\eta_t) \leq 0$. 
%Let $\pi$ be the uniform distribution over $\{1,\hdots,K\}$ and 
Let
\[
    p_{i,t}^{\eta}=\frac{\exp{\left(- \eta \tilde{L}_{i,t}\right)}}{\sum_{k=1}^K \exp{\left(- \eta \tilde{L}_{k,t}\right)}}~.
\]
Then
\begin{align*}
\Phi_t'(\eta) & = - \frac{1}{\eta^2} \ln\left(\frac{1}{K} \sum_{i=1}^K \exp{\left(- \eta \tilde{L}_{i,t}\right)}\right) - \frac{1}{\eta}\frac{\sum_{i=1}^K \tilde{L}_{i,t} \exp{\left(- \eta \tilde{L}_{i,t}\right)}}{\sum_{i=1}^K \exp{\left(- \eta \tilde{L}_{i,t}\right)}} \\*
& = \frac{1}{\eta^2} \frac{1}{\sum_{i=1}^K \exp{\left(- \eta \tilde{L}_{i,t}\right)}} \sum_{i=1}^K \exp{\left(- \eta \tilde{L}_{i,t}\right)} \\*
& \qquad \times \left(-\eta \tilde{L}_{i,t} - \ln\left(\frac{1}{K} \sum_{i=1}^K \exp{\left(- \eta \tilde{L}_{i,t}\right)}\right) \right)~.
\end{align*}
Simplifying, we get (since $p_1$ is the uniform distribution over $\{1,\hdots,K\}$),
\[
\Phi_t'(\eta) = \frac{1}{\eta^2} \sum_{i=1}^K p_{i,t}^{\eta} \ln(K p_{i,t}^{\eta}) = \frac{1}{\eta^2} \K(p_t^{\eta}, p_1) \geq 0~.
\]
\end{proof}

\section{High probability and expected regret bounds} \label{sec:regret}
In this section we prove a high probability bound on the regret. Unfortunately, the Exp3 strategy defined in the previous section is not adequate for this task. Indeed, the variance of the estimate $\tilde{\ell}_{i,t}$ is of order $1/p_{i,t}$, which can be arbitrarily large. In order to ensure that the probabilities $p_{i,t}$ are bounded from below, the original version of Exp3 mixes the exponential weights with a uniform distribution over the arms. In order to avoid increasing the regret, the mixing coefficient $\gamma$ associated with the uniform distribution cannot be larger than $n^{-1/2}$. Since this implies that the variance of the cumulative loss estimate $\tilde{L}_{i,n}$ can be of order $n^{3/2}$, very little can be said about the concentration of the regret also for this variant of Exp3.

This issue can be solved by combining the mixing idea with a different estimate for losses. In fact, the core idea is more transparent when expressed in terms of gains, and so we turn to the gain version of the problem. The trick is to introduce a bias in the gain estimate which allows to derive a high probability statement on this estimate.
%
\begin{lemma} \label{lem:exp3P}
For $\beta \le 1$, let
\[
    \tilde{g}_{i,t} = \frac{g_{i,t} \ds1_{I_t=i} + \beta}{p_{i,t}}~.
\]
Then, with probability at least $1-\delta$,
$$\sum_{t=1}^n g_{i,t} \leq \sum_{t=1}^n \tilde{g}_{i,t}  + \frac{\ln(\delta^{-1})}{\beta}~.$$
\end{lemma}
%
\begin{proof}
Let $\E_t$ be the expectation conditioned on $I_1,\dots,I_{t-1}$. Since $\exp(x)\le 1+x+x^2$ for $x\leq 1$, for $\beta\leq 1$ we have
\begin{align*}
    \E_t \exp&\bigg(\beta g_{i,t} - \beta \frac{g_{i,t} \ds1_{I_t=i} + \beta}{p_{i,t}}\bigg)
\\ &\leq
    \Bigg(1 + \E_t \bigg[\beta g_{i,t} - \beta \frac{g_{i,t} \ds1_{I_t=i}}{p_{i,t}}\bigg] + \E_t \bigg[\beta g_{i,t} - \beta \frac{g_{i,t} \ds1_{I_t=i}}{p_{i,t}}\bigg]^2 \Bigg)
\\ &\quad
    \times\exp\bigg(\!- \frac{\beta^2}{p_{i,t}}\bigg)
\\ &\leq
    \Bigg(1 + \beta^2 \frac{g^2_{i,t}}{p_{i,t}} \Bigg) \exp\bigg(\!- \frac{\beta^2}{p_{i,t}}\bigg)
\\ &\leq
     1
\end{align*}
where the last inequality uses $1+u \leq \exp(u)$. As a consequence, we have
  \begin{align*}
  \E \exp\bigg(\beta \sum_{t=1}^n g_{i,t} - \beta \sum_{t=1}^n \frac{g_{i,t} \ds1_{I_t=i} + \beta}{p_{i,t}}\bigg) \leq 1.
  \end{align*}
Moreover, Markov's inequality implies $\P\left(X>\ln(\delta^{-1})\right) \leq \delta \E e^X$ and thus, with probability at least $1-\delta$,
$$\beta \sum_{t=1}^n g_{i,t} - \beta \sum_{t=1}^n \frac{g_{i,t} \ds1_{I_t=i} + \beta}{p_{i,t}} \leq \ln(\delta^{-1})~.$$
\end{proof}
%
\begin{figure}[t]
\bookbox{
{\em Exp3.P}
\\
{Parameters:} $\eta \in \R^+$ and $\gamma, \beta \in [0,1]$.

\smallskip\noindent
Let $p_1$ be the uniform distribution over $\{1,\hdots,K\}$.

\smallskip\noindent
For each round $t=1,2,\ldots,n$
\begin{itemize}
\item[(1)]
Draw an arm $I_t$ from the probability distribution $p_t$.
\item[(2)]
Compute the estimated gain for each arm:
\[
    \tilde{g}_{i,t} = \frac{g_{i,t} \ds1_{I_t = i} + \beta}{p_{i,t}}
\]
and update the estimated cumulative gain: $\tilde{G}_{i,t} = \sum_{s=1}^t \tilde{g}_{i,s}$.
\item[(3)]
Compute the new probability distribution over the arms $p_{t+1}=(p_{1,t+1},\hdots,p_{K,t+1})$ where:
$$p_{i,t+1} = (1-\gamma) \frac{\exp{\left(\eta \tilde{G}_{i,t}\right)}}{\sum_{k=1}^K \exp{\left(\eta \tilde{G}_{k,t}\right)}} + \frac{\gamma}{K}.$$
\end{itemize}
}
\caption{Exp3.P forecaster.}
\label{fig:exp3P}
\end{figure}

The strategy associated with these new estimates, called Exp3.P, is described in Figure \ref{fig:exp3P}. Note that, for the sake of simplicity, the strategy is described in the setting with known time horizon ($\eta$ is constant). Anytime results can easily be derived with the same techniques as in the proof of Theorem~\ref{th:Exp3}. 

In the next theorem we propose two different high probability bounds. In \eqref{eq:exp3Pbound1} the algorithm needs the confidence level $\delta$ as an input parameter. In~\eqref{eq:exp3Pbound2} the algorithm satisfies a high probability bound for any confidence level. This latter property is particularly important to derive good bounds on the expected regret.
%
\begin{theorem}[High probability bound for Exp3.P] \label{th:exp3P}
For any given $\delta \in (0,1)$, if Exp3.P is run with
\[
    \beta = \sqrt{\frac{\ln(K \delta^{-1})}{n K}}, \quad \eta = 0.95 \sqrt{\frac{\ln(K)}{n K}}, \quad \gamma = 1.05 \sqrt{\frac{K \ln(K)}{n}}
\]
then, with probability at least $1-\delta$,
\begin{equation} \label{eq:exp3Pbound1}
R_n \leq 5.15 \sqrt{n K \ln(K \delta^{-1})}~.
\end{equation}
Moreover, if Exp3.P is run with
$
    \beta = \sqrt{\tfrac{\ln(K)}{n K}} %, \quad \eta = 0.95 \sqrt{\frac{\ln(K)}{n K}}, \quad \gamma = 1.05 \sqrt{\frac{K \ln(K)}{n}}
$
while $\eta$ and $\gamma$ are chosen as before, then, with probability at least $1-\delta$,
\begin{equation} \label{eq:exp3Pbound2}
R_n \leq \sqrt{\frac{n K}{\ln(K)}} \ln(\delta^{-1}) + 5.15 \sqrt{n K \ln(K)}~.
\end{equation}
\end{theorem}
%
\begin{proof}
We first prove (in three steps) that if $\gamma \leq 1/2$ and $(1+\beta) K \eta \leq \gamma$, then Exp3.P satisfies, with probability at least $1-\delta$,
\begin{equation} \label{eq:exp3Plem}
R_n \leq \beta n K + \gamma n + (1+\beta) \eta K n + \frac{\ln(K \delta^{-1})}{\beta} + \frac{\ln K}{\eta}~.
\end{equation}

\subsection*{First step: Notation and simple equalities.}
%
One can immediately see that $\E_{i \sim p_t} \tilde{g}_{i,t} = g_{I_t,t} + \beta K$, and thus
%
\begin{equation} \label{eq:exp3P1}
\sum_{t=1}^n g_{k,t} - \sum_{t=1}^n g_{I_t,t} = \beta n K  + \sum_{t=1}^n g_{k,t} - \sum_{t=1}^n \E_{i \sim p_t} \tilde{g}_{i,t}~.
\end{equation}
%
The key step is, again, to consider the cumulant-generating function of $\tilde{g}_{i,t}$. However, because of the mixing, we need to introduce a few more notations. Let $u=\bigl(\tfrac{1}{K},\hdots,\tfrac{1}{K}\bigr)$ be the uniform distribution over the arms, let and $w_t = \tfrac{p_t - u}{1-\gamma}$ be the distribution induced by Exp3.P at time $t$ without the mixing. Then we have:
\begin{align} 
- \E_{i \sim p_t} \tilde{g}_{i,t} & = - (1-\gamma) \E_{i \sim w_t} \tilde{g}_{i,t} - \gamma \E_{i \sim u} \tilde{g}_{i,t} \notag \\
& = (1-\gamma) \biggl( \frac{1}{\eta} \ln \E_{i \sim w_t} \exp \bigl( \eta (\tilde{g}_{i,t} - \E_{k \sim w_t} \tilde{g}_{k,t}) \bigr) \notag \\
& \quad - \frac{1}{\eta} \ln \E_{i \sim w_t} \exp\left( \eta \tilde{g}_{i,t} \right) \biggr) - \gamma \E_{i \sim u} \tilde{g}_{i,t}~. \label{eq:exp3P2} 
\end{align}

\subsection*{Second step: Study of the first term in \eqref{eq:exp3P2}.}
We use the inequalities $\ln x \leq x-1$ and $\exp(x) \leq 1 + x + x^2$, for all $x \leq 1$, as well as the fact that $\eta \tilde{g}_{i,t} \leq 1$ since $(1+\beta) \eta K \leq \gamma$:
\begin{align}
\ln \E_{i \sim w_t} \exp{\Bigl( \eta \bigl(\tilde{g}_{i,t} - \E_{k \sim p_t} \tilde{g}_{k,t}\bigr) \Bigr)} \notag
& = \ln \E_{i \sim w_t} \exp{\bigl( \eta \tilde{g}_{i,t}\bigr)} - \eta\,\E_{k \sim p_t} \tilde{g}_{k,t} \notag \\
&\leq \E_{i \sim w_t} \Bigl[\exp{\bigl( \eta \tilde{g}_{i,t}\bigr)} - 1 - \eta \tilde{g}_{i,t} \Bigr] \notag \\
&\leq \E_{i \sim w_t} \eta^2 \tilde{g}_{i,t}^2  \notag \\
&\leq \frac{1+\beta}{1-\gamma} \eta^2 \sum_{i=1}^K \tilde{g}_{i,t} \label{eq:exp3P3}
\end{align}
where we used $\frac{w_{i,t}}{p_{i,t}} \leq \frac{1}{1-\gamma}$ in the last step.

\subsection*{Third step: Summing.}
%
Set $\tilde{G}_{i,0} = 0$. Recall that $w_t=\bigl(w_{1,t},\hdots,w_{K,t}\bigr)$ with 
\begin{equation} \label{eq:exp3P4}
w_{i,t} = \frac{\exp{\left(- \eta \tilde{G}_{i,t-1}\right)}}{\sum_{k=1}^K \exp{\left(- \eta \tilde{G}_{k,t-1}\right)}}~.
\end{equation}
Then substituting \eqref{eq:exp3P3} in \eqref{eq:exp3P2} and summing using \eqref{eq:exp3P4}, we obtain
%
\begin{align*}
- \sum_{t=1}^n &\E_{i \sim p_t} \tilde{g}_{i,t} \\
& \leq (1+\beta) \eta \sum_{t=1}^n \sum_{i=1}^K \tilde{g}_{i,t} - \frac{1-\gamma}{\eta} \sum_{t=1}^n \ln \left( \sum_{i=1}^K w_{i,t} \exp{\left( \eta \tilde{g}_{i,t} \right)}  \right) \\
& = (1+\beta) \eta \sum_{t=1}^n \sum_{i=1}^K \tilde{g}_{i,t} - \frac{1-\gamma}{\eta} \ln \left( \prod_{t=1}^n \frac{\sum_{i=1}^K \exp(\eta \tilde{G}_{i,t})}{\sum_{i=1}^K \exp(\eta \tilde{G}_{i,t-1})} \right) \\
& \leq (1+\beta) \eta K \max_j \tilde{G}_{j,n} + \frac{\ln K}{\eta} - \frac{1-\gamma}{\eta} \ln \left( \sum_{t=1}^n \exp(\eta \tilde{G}_{i,n}) \right) \\
& \leq - \bigl(1 - \gamma - (1+\beta) \eta K\bigr) \max_j \tilde{G}_{j,n} + \frac{\ln(K)}{\eta} \\
& \leq - \bigl(1 - \gamma - (1+\beta) \eta K\bigr) \max_j \sum_{t=1}^n g_{j,t} + \frac{\ln(K \delta^{-1})}{\beta} + \frac{\ln(K)}{\eta}~.
\end{align*}
The last inequality comes from Lemma \ref{lem:exp3P}, the union bound, and $\gamma - (1+\beta) \eta K \leq 1$ which is a consequence of $(1+\beta) \eta K \leq \gamma \leq 1/2$. Combining this last inequality with \eqref{eq:exp3P1} we obtain
$$R_n \leq \beta n K + \gamma n + (1+\beta) \eta K n + \frac{\ln\bigl(K \delta^{-1}\bigr)}{\beta} + \frac{\ln(K)}{\eta}$$
which is the desired result.

Inequality~\eqref{eq:exp3Pbound1} is then proved as follows. First, it is trivial if $n \geq 5.15 \sqrt{n K \ln(K \delta^{-1})}$ and thus we can assume that this is not the case. This implies that $\gamma \leq 0.21$ and $\beta \leq 0.1$, and thus we have $(1+\beta) \eta K \leq \gamma \leq 1/2$. Using \eqref{eq:exp3Plem} directly yields the claimed bound. The same argument can be used to derive \eqref{eq:exp3Pbound2}.
\end{proof}
%
We now discuss expected regret bounds. As the cautious reader may already have observed, if the adversary is oblivious, namely when $\bigl(\ell_{1,t}, \hdots, \ell_{K,t}\bigr)$ is independent of $I_1, \hdots, I_{t-1}$ for each $t$, a pseudo-regret bound implies the same bound on the expected regret. This follows from noting that the expected regret against an oblivious adversary is smaller than the maximal pseudo-regret against deterministic adversaries, see \cite[Proposition 33]{AB10} for a proof of this fact. In the general case of a non-oblivious adversary, the loss vector $\bigl(\ell_{1,t}, \hdots, \ell_{K,t}\bigr)$ at time $t$ depends on the past actions of the forecaster. This makes the analysis of the expected regret more intricate. One way around this difficulty is to first prove high probability bounds, and then integrate the resulting bound. Following this method, we derive a bound on the expected regret of Exp3.P using \eqref{eq:exp3Pbound2}.
%
\begin{theorem}[Expected regret of Exp3.P]
If Exp3.P is run with
\[
    \beta = \sqrt{\frac{\ln K }{n K}}, \quad \eta = 0.95 \sqrt{\frac{\ln K }{n K}}, \quad \gamma = 1.05 \sqrt{\frac{K \ln K }{n}}
\]
then
\begin{equation} \label{eq:exp3Pbound3}
\E\,R_n \leq 5.15 \sqrt{n K \ln K} + \sqrt{\frac{n K}{\ln K }}~.
\end{equation}
\end{theorem}
%
\begin{proof}
We integrate the deviations in \eqref{eq:exp3Pbound2} using the formula
\[
    \E\,W \leq \int_{0}^1 \frac{1}{\delta} \P\left(W>\ln\frac{1}{\delta}\right) d\delta
\]
for a real-valued random variable $W$. In particular, taking
\[
    W=\sqrt{\frac{\ln K }{n K}} \left(R_n - 5.15 \sqrt{n K \ln K }\right)
\]
yields $\E\,W \leq 1$, which is equivalent to \eqref{eq:exp3Pbound3}.
\end{proof}

\section{Lower Bound} \label{sec:LB}
The next theorem shows that the results of the previous sections are essentially unimprovable, up to logarithmic factors. The result is proven via the probabilistic method: we show that there exists a distribution of rewards for the arms such that the pseudo-regret of any forecaster must be high when averaged over this distribution. Owing to this probabilistic construction, the lower bound proof is based on the same Kullback-Leibler divergence as the one used in the proof of the lower bound for stochastic bandits ---see Subsection~\ref{s:stoch-lower}. We are not aware of other techniques for proving bandit lower bounds.

We find it more convenient to prove the results for rewards rather than losses. In order to emphasize that our rewards are stochastic (in particular, Bernoulli random variables), we use $Y_{i,t}\in\{0,1\}$ to denote the reward obtained by pulling arm $i$ at time $t$.
%
\begin{theorem}[Minimax lower bound] \label{th:LBminimax}
Let $\sup$ be the supremum over all distribution of rewards such that, for $i=1,\dots,K$, the rewards $Y_{i,1},Y_{i,2},\ldots\in\{0,1\}$ are i.i.d., and let $\inf$ be the infimum over all forecasters. Then
\begin{equation} \label{eq:LBminimax}
\inf \sup \left(\max_{i=1,\hdots,K} \E \sum_{t=1}^n Y_{i,t} - \E \sum_{t=1}^n Y_{I_t,t} \right) \geq \frac{1}{20} \sqrt{n K}
\end{equation}
where expectations are with respect to both the random generation of rewards and the internal randomization of the forecaster.
%and 
%\begin{equation} \label{eq:LBminimaxasymp}
%\sup_{n, K} \frac{\inf \sup \oR_n}{\sqrt{n K}} \geq \frac{1}{4}.
%\end{equation}
\end{theorem}
%
Since $\max_{i=1,\hdots,K} \E \sum_{t=1}^n Y_{i,t} - \E \sum_{t=1}^n Y_{I_t,t} = \oR_n \le \E\,R_n$, Theorem~\ref{th:LBminimax} immediately entails a lower bound on the regret of any forecaster.

The general idea of the proof goes as follows. Since at least one arm is pulled less than $n/K$ times, for this arm one cannot differentiate between a Bernoulli of parameter $1/2$ and and a Bernoulli of parameter $1/2+\sqrt{K/n}$. Thus, if all arms are Bernoulli of parameter $1/2$ but one, whose parameter is $1/2+\sqrt{K/n}$, then the forecaster should incur a regret of order $n \sqrt{K/n} = \sqrt{n K}$. In order to formalize this idea, we use the Kullback-Leibler divergence, and in particular Pinsker's inequality, to compare the behavior of a given forecaster against: (1) the distribution where all arms are Bernoulli of parameter $1/2$; (2) the same distribution where the parameter of one arm is increased by $\epsilon$. 

We start by proving a more general lemma, which could also be used to derive lower bounds in other contexts. The proof of Theorem \ref{th:LBminimax} then follows by a simple optimization over $\epsilon$.
%
\begin{lemma} \label{lem:LBminimax}
Let $\epsilon \in [0,1)$. For any $i \in \{1,\hdots, K\}$ let $\E_i$ be the expectation against the joint distribution of rewards where all arms are i.i.d.\ Bernoulli of parameter $\frac{1-\epsilon}{2}$ but arm $i$, which is i.i.d.\ Bernoulli of parameter $\frac{1+\epsilon}{2}$. Then, for any forecaster,
$$\max_{i = 1, \hdots, K}  \E_i \sum_{t=1}^n \bigl(Y_{i,t} - Y_{I_t,t}\bigr) \geq n \epsilon \left(1 - \frac{1}{K} - \sqrt{\epsilon \ln\frac{1+\epsilon}{1-\epsilon}}  \sqrt{\frac{n}{2 K}}\right)~.$$
\end{lemma}
%
\begin{proof}
We provide a proof in five steps by lower bounding $\frac{1}{K} \sum_{i=1}^K \E_i \sum_{t=1}^n (Y_{i,t} - Y_{I_t,t})$. This implies the statement of the lemma because a max is larger than a mean.
%
\subsection*{First step: Empirical distribution of plays.}
%
We start by considering a deterministic forecaster. Let $q_n=\bigl(q_{1,n},\hdots,q_{K,n}\bigr)$ be the empirical distribution of plays over the arms defined by $q_{i,n} = \frac{T_i(n)}{n}$ ---recall from Chapter~\ref{stochastic} that $T_i(n)$ denotes the number of times arm $i$ was selected in the first $n$ rounds. Let $J_n$ be drawn according to $q_n$. We denote by $\P_i$ the law of $J_n$ against the distribution where all arms are i.i.d.\ Bernoulli of parameter $\frac{1-\epsilon}{2}$ but arm $i$, which is i.i.d.\ Bernoulli of parameter $\frac{1+\epsilon}{2}$ (we call this the $i$-th stochastic adversary). Recall that $\P_{i}(J_n = j) = \E_i\tfrac{T_j(n)}{n}$, hence
$$\E_i \sum_{t=1}^n \bigl(Y_{i,t} - Y_{I_t,t}\bigr) = \epsilon n \sum_{j \neq i} \P_{i}(J_n = j) = \epsilon n \bigl(1 - \P_{i}(J_n = i)\bigr)$$
which implies
\begin{equation} \label{eq:firststepmm}
\frac{1}{K} \sum_{i=1}^K \E_i \sum_{t=1}^n \bigl(Y_{i,t} - Y_{I_t,t}\bigr) = \epsilon n \left(1 - \frac{1}{K} \sum_{i=1}^K \P_{i}(J_n = i)\right)~.
\end{equation}

\subsection*{Second step: Pinsker's inequality.}
%
Let $\P_0$ be the law of $J_n$ for the distribution where all arms are i.i.d.\ Bernoulli of parameter $\frac{1-\epsilon}{2}$. Then Pinsker's inequality immediately gives $\P_{i}(J_n = i) \le \P_{0}(J_n = i) + \sqrt{\tfrac{1}{2} \K(\P_{0},\P_{i})}$, and so
\begin{equation} \label{eq:secondstepmm}
\frac{1}{K} \sum_{i=1}^K \P_{i}(J_n = i) \leq \frac{1}{K} +  \frac{1}{K} \sum_{i=1}^K \sqrt{\frac{1}{2} \K(\P_{0}, \P_{i})}~.
\end{equation}

\subsection*{Third step: Computation of $\K(\P_{0}, \P_{i})$.
% with the chain rule for Kullback-Leibler divergence.
}
%
Since the forecaster is deterministic, the sequence of rewards $Y^n = (Y_1,\hdots,Y_n) \in \{0,1\}^n$ received by the forecaster uniquely determines the empirical distribution of plays $q_n$. In particular, the law of $J_n$ conditionally to $Y^n$ is the same for any $i$-th stochastic adversary. For each $i=0, \hdots, K$, let $\P_i^n$ be the law of $Y^n$ against the $i$-th adversary. Then one can easily show that 
$
%\label{eq:thirdstep1mm}
\K(\P_{0}, \P_{i}) \leq \K(\P_{0}^n, \P_{i}^n)
$.
Now we use the chain rule for Kullback-Leibler divergence ---see for example \cite[Section A.2]{CL06}--- iteratively to introduce the laws $\P^t_i$ of $Y^t=(Y_1, \hdots, Y_t)$. More precisely, we have
%
\begin{align}
\lefteqn{\K(\P_{0}^n, \P_{i}^n)} \notag \\
& = \K(\P_0^1, \P_i^1) + \sum_{t=2}^n \sum_{y^{t-1}} \P_0^{t-1}(y^{t-1})\,\K\bigl(\P_{0}^t(\cdot\mid y^{t-1}),\P_{i}^t(\cdot\mid y^{t-1})\bigr) \notag \\
& = \K(\P_0^1, \P_i^1) + \sum_{t=2}^n \left( \sum_{y^{t-1} \,:\, I_t = i} \P_0^{t-1}(y^{t-1})\,\K\left(\tfrac{1-\epsilon}{2},\tfrac{1+\epsilon}{2}\right) \right. \notag \\
&\qquad\qquad\qquad\qquad\qquad
+ \left. \sum_{y^{t-1} \,:\, I_t \neq i} \P_0^{t-1}(y^{t-1})\,\K\left(\tfrac{1+\epsilon}{2}, \tfrac{1+\epsilon}{2}\right)\right) \notag \\
& = \K\left(\tfrac{1-\epsilon}{2}, \tfrac{1+\epsilon}{2} \right) \E_0\,T_{i}(n)~. \label{eq:thirdstep2mm}
\end{align}
%
\subsection*{Fourth step: conclusion for deterministic forecasters.
% with the concavity of the square root.
}
%
By using that the square root is concave, and combining $\K(\P_{0}, \P_{i}) \leq \K(\P_{0}^n, \P_{i}^n)$ with \eqref{eq:thirdstep2mm}, we deduce that
\begin{align}
\frac{1}{K} \sum_{i=1}^K \sqrt{\K(\P_{0},\P_{i})}& \leq \sqrt{\frac{1}{K} \sum_{i=1}^K \K(\P_{0},\P_{i})} \notag \\
& \leq \sqrt{\frac{1}{K} \sum_{i=1}^K \K\left(\tfrac{1-\epsilon}{2}, \tfrac{1+\epsilon}{2} \right) \E_0\,T_{i}(n)} \notag \\
& = \sqrt{\frac{n}{K} \K\left(\tfrac{1-\epsilon}{2}, \tfrac{1+\epsilon}{2} \right)}~. \label{eq:fourthstepmm}
\end{align}
We conclude the proof for deterministic forecasters by applying \eqref{eq:secondstepmm} and \eqref{eq:fourthstepmm} to \eqref{eq:firststepmm}, and observing that
$
\K\left(\tfrac{1-\epsilon}{2}, \tfrac{1+\epsilon}{2} \right) = 
%\frac{1-\epsilon}{2} \ln\frac{1-\epsilon}{1+\epsilon} + \frac{1+\epsilon}{2} \ln\frac{1+\epsilon}{1-\epsilon} =
\epsilon \ln\tfrac{1+\epsilon}{1-\epsilon}
$.

\subsection*{Fifth step: randomized forecasters via Fubini's Theorem.}
%
Extending previous results to randomized forecasters is easy. Denote by $\E_r$ the expectation with respect to the forecaster's internal randomization. Then Fubini's Theorem implies
$$
\frac{1}{K} \sum_{i=1}^K \E_i\sum_{t=1}^n \E_r\bigl(Y_{i,t} - Y_{I_t,t}\bigr) = \E_r \frac{1}{K} \sum_{i=1}^K \E_i \sum_{t=1}^n \bigl(Y_{i,t} - Y_{I_t,t}\bigr)~.$$
Now the proof is concluded by applying the lower bound on $\tfrac{1}{K} \sum_{i=1}^K \E_i \sum_{t=1}^n \bigl(Y_{i,t} - Y_{I_t,t}\bigr)$, which we proved in previous steps, to each realization of the forecaster's random bits.
\end{proof}

\section{Refinements and bibliographic remarks}
The adversarial framework studied in this chapter was originally investigated in a {\em full information} setting, where at the end of each round the forecaster observes the complete loss vector $(\ell_{1,t}, \hdots, \ell_{K,t})$. We refer the reader to \cite{CL06} for the history of this problem. The Exp3 and Exp3.P strategies were introduced\footnote{In its original formulation the Exp3 strategy was defined as a mixture of exponential weights with the uniform distribution on the set of arms. It was noted in \cite{Sto05} that this mixing is not necessary, see footnote 2 on p26 in \cite{Bub10} for more details on this.} and analyzed by \cite{ACFS03}, where the lower bound of Theorem \ref{th:LBminimax} is also proven. The proofs presented in this chapter are taken from \cite{Bub10}. We now give an overview of some of the many improvements and refinements that have been proposed since these initial analyses.

\subsection{Log-free upper bounds} \label{sec:logfree}
One can see that there is a logarithmic gap between the pseudo-regret of Exp3, presented in Theorem \ref{th:Exp3}, and the minimax lower bound of Theorem \ref{th:LBminimax}. This gap was closed by \cite{AB09}, who constructed a new class of strategies and showed that some of them have a pseudo-regret of order $\sqrt{n K}$. This new class of strategies, called INF (Implicitily Normalized Forecaster), is based on the following idea. First, note that one can generalize the exponential weighting scheme of Exp3 as follows: given a potential function $\psi$, assign the probability 
\[
    p_{i,t+1}=\frac{\psi(\tilde{L}_{i,t})}{\sum_{j=1}^K \psi(\tilde{L}_{j,t})}~.
\]
This type of strategy is called a weighted average forecaster, see \cite[Chapter 2]{CL06}. In INF the normalization is done implicitily, by a translation of the losses. More precisely, INF with potential $\psi$ assigns the probability $p_{i,t+1} = \psi\bigl(C_t - \tilde{L}_{i,t}\bigr)$, where $C_t$ is the constant such that $p_{t+1}$ sum to $1$. The key to obtain a minimax optimal pseudo-regret is to take $\psi$ of the form $\psi(x) = (- \eta x)^{-q}$ with $q>1$, while Exp3 corresponds to $\psi(x) = \exp(\eta x)$. \cite{ABL11} realized that the INF strategy can be formulated as a Mirror Descent algorithm. This point of view significantly simplifies the proofs. We refer the reader to Chapter~\ref{linear} (and in particular Theorem~\ref{th:osmdzero}) for more details.

While it is possible to get log-free pseudo-regret bounds, the situation becomes significantly more complicated when one considers high probability regret and expected regret. \cite{AB10} proved that one can get a log-free expected regret if the adversary is oblivious, i.e., the sequence of loss vectors is independent of the forecaster's actions. Moreover, it is also possible to get a log-free high probability regret if the adversary is fully oblivious (i.e., the loss vectors are independently drawn, but not identically distributed ---this includes the oblivious adversary). It is conjectured (in \cite{AB10}) that it is not possible to obtain a log-free expected regret bound against a general non-oblivious adversary.

\subsection{Adaptive bounds}
One of the strengths of the bounds proposed in this chapter is also one of its weaknesses: the bounds hold against any adversary. It is clear that in some cases it is possible to obtain a much smaller regret than the worst case regret. For example, when the sequence of losses is an i.i.d.\ sequence, we proved in Chapter \ref{stochastic} that it is is possible to obtain a logarithmic pseudo-regret (provided that the gap $\Delta$ is considered as a constant). Thus it is natural to ask if it possible to have strategies with minimax optimal regret, but also with much smaller regret when the loss sequence is not worst case.

The first bound in this direction was proved by \cite{ACFS03}, who showed that, for the gain version of the problem and against an oblivious adversary, Exp3 has a pseudo-regret of order $\sqrt{K G_n^*}$ (omitting log factors), where $G_n^* \leq n$ is the maximal cumulative reward of the optimal arm after $n$ rounds. This result was improved by \cite{AB10}, who showed that using the gain estimate
\[
    \tilde{g}_{i,t} = - \frac{\ds1_{I_t = i}}{\beta} \ln \left(1 - \frac{\beta g_{i,t}}{p_{i,t}} \right)
\]
one can bound the regret with high probability by essentially the same quantity as before, and against any adversary.

Another direction was explored by \cite{HK09} building on previous works in the full information setting ---see \cite{CMS07}. In this work the authors proved that one can attain a regret of order $\sqrt{\sum_{i=1}^K V_{i,n}}$ excluding log factors, where
\[
    V_{i,n} = \sum_{t=1}^n \left(\ell_{i,t} - \frac{1}{n} \sum_{s=1}^n \ell_{i,s} \right)^2
\]
is the total variation of the loss for arm $i$. In fact their result is more general, as it applies to the linear bandit framework ---see Chapter~\ref{linear}. The main new ingredient in their analysis is a ``reservoir sampling'' procedure. We refer the reader to \cite{HK09} for details. See also the works of \cite{slivkins2008adapting,slivkins2009contextual} for related results on slowly changing bandits.

In Section \ref{sec:best-of-both} below we describe another type of adaptive bound, where one combines minimax optimal regret for the adversarial model with logarithmic pseudo-regret for the stochastic model.


\subsection{Competing with the best switching strategy} \label{sec:exp3S}
While competing against the policy consistently playing the best fixed arm is a natural way of defining regret, in some applications it might be interesting to consider regret with respect to a bigger class of policies. Though this problem is the focus of Chapter~\ref{contextual}, there is a class of natural policies that can be directly dealt with by the methods of this chapter. Namely, consider the problem of competing against any policy constrained to make at most $S \leq n$ switches (a switch is when the arm played at time $t$ is different from the arm played at time $t+1$). This problem was studied by \cite{Aue02}, where it was first shown that a simple variant of Exp3 attains a low switching regret against oblivious adversaries. Later, \cite{AB10} proved that Exp3.P attains an expected regret (and a high probability regret) of order $\sqrt{n K S \ln (n K /S)}$ for this problem.

\subsection{Stochastic versus adversarial bandits} \label{sec:best-of-both}
From a practical viewpoint, Exp3 should be a safe choice when we have reasons to believe that the sequence of rewards is not well matched by any i.i.d.\ process. Indeed, it is easy to prove that UCB can have linear regret, i.e. $\oR_n = \Omega(n)$, on certain deterministic sequences. In \cite{BS12} a new strategy was described, called SAO (Stochastic and Adversarial Optimal), which enjoys (up to logarithmic factors) both the guarantee of Exp3 for the adversarial model and the guarantee of UCB for the stochastic model. More precisely SAO satisfies $\oR_n = \mathcal{O} \left(\frac{K}{\Delta} \log^2(n) \log(K) \right)$ in the stochastic model and $\oR_n = \mathcal{O} \left( \sqrt{n K} \log^{3/2}(n) \log(K) \right)$ in the adversarial model. Note that while this result is a step towards more flexible strategies, the very notion of regret $\oR_n$ can become vacuous with nonstationarities in the reward sequence, since the total reward of the best fixed action might be very small. In that case the notion of switching regret ---see Subsection \ref{sec:exp3S}--- is more relevant, and it would be interesting to derive a strategy with logarithmic regret in the stochastic model, and a switching regret of order $\sqrt{n K S}$ in the adversarial model.
 %Nonstationarities, which make the standard notion of regret vacuous by reducing the total reward of the optimal action, cause problems to both UCB and Exp3. In this case, variants of Exp3 with a good switching regret ---see Subsection \ref{sec:exp3S}--- are a reasonable option to obtain good practical performances.
%\textbf{[to be expanded]}



\subsection{Alternative feedback structures} \label{sec:feedback}
As mentioned at the beginning of this section, the adversarial multi-armed bandit is a variation of the full information setting, with a weaker feedback signal (only the incurred loss versus the full vector of losses is observed). Many other feedback structures can be considered, and we conclude the chapter by describing a few of them.

In the {\em label efficient} setting, originally proposed by \cite{HP97}, at the end of each round the forecaster has to decide whether to ask for the losses of the current round, knowing that this can be done for at most $m \leq n$ times. In this setting, \cite{CLS05} proved that the minimax pseudo-regret is of order $n \sqrt{\frac{\ln K}{m}}$. A bandit label efficient version was proposed by \cite{AAGO06}. \cite{AB10} proved that the minimax pseudo-regret for the bandit label efficient version is of order $n \sqrt{\frac{K}{m}}$. These results do not require any fundamentally new algorithmic idea, besides the fact the forecaster has to randomize to select the rounds in which the losses are revealed. Roughly speaking, a simple coin toss with parameter $\epsilon = m/n$ is sufficient to obtain an optimal regret.
%A significant generalization of the label efficient and bandit settings is {\em partial monitoring}. We refer the reader to \cite[Chapter 6]{CL06} for more details.

\cite{MS11} study a model that interpolates between the full information and the bandit setting. The basic idea is that there is an undirected graph $G$ with $K$ vertices (one vertex for each arm) that encodes the feedback structure. When one pulls arm $i$ the losses of all neighboring arms $j \in N(i)$ in the graph are observed. Thus, a graph with no edges is equivalent to the bandit problem, while the complete graph is equivalent to the full information setting. Given the feedback structure $G$, it is natural to consider the following unbiased loss estimate
\[
    \tilde{\ell}_{i,t} = \frac{\ell_{i,t} \ds1_{i \in N(I_t)}}{\sum_{j \in N(i)} p_{j,t}}~.
\]
Using Exp3 with this loss estimate, the authors show that the minimax pseudo-regret (up to logarithmic factors) is of order of $\sqrt{\alpha(G) n}$, where $\alpha(G)$ is the independence number of graph $G$. Note that this interpolated setting naturally arises in applications like ad placement on websites. Indeed, if a user clicks on an advertisement, it is plausible to assume that the same user would have clicked on similar advertisements, had they been displayed.

The above problems are all specific examples of the more general partial monitoring setting. In this model, at the end of each round the player does not observe the incurred loss $\ell_{I_t,t}$ but rather a stochastic ``signal'' $S_{I_t,t}$. A prototypical example of this scenario is the following: a website is repeatedly selling the same item to a sequence of customers. The selling price is dynamically adjusted, and each customer buys the item only if the current price is smaller or equal than his own hidden value for the item. The pricing algorithm (i.e., the player in our terminology) does not see each user's value, but only whether the user bought the item or not.

The relationship between the signals and the incurred losses defines the instance of a partial monitoring problem. We refer the interested reader to \cite{CL06} for more details, including an historical account. Recent progress on this problem has been made by \cite{BPS10} and~\cite{FR11}.
